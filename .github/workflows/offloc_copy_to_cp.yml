---
name: NDH Offloc Copy to Cloud Platform

on:
  workflow_dispatch:
    inputs:
      dry_run:
        type: choice
        description: Run in dry run mode (leave as false unless testing)
        default: "true" # Default to true for testing
        options:
          - "true"
          - "false"

#  schedule:
    # Add this later
    # requirements - check if file exists at 01:00
    # check again at - 07:00
    # check again at - 09:00
    # Need to check if file is actually correct date FIRST!

permissions:
  id-token: write
  contents: read

run-name: "NDH Offloc Copy to Cloud Platform (${{ inputs.dry_run == 'true' && ' dry run)' || ')' }}"

jobs:
  setup:
    name: Setup
    runs-on: ubuntu-latest
    outputs:
      dry_run: "${{ steps.parseinput.outputs.dry_run }}"
    steps:
      - name: Checkout Repository
        uses: actions/checkout@0ad4b8fadaa221de15dcec353f45205ec38ea70b  # v4.1.4
        with:
          ref: ${{ github.ref }}

      - name: Parse Workflow Inputs
        id: parseinput
        run: |
          dry_run="${{ github.event.inputs.dry_run }}"
          echo "::set-output name=dry_run::${dry_run}"
          if [[ "${dry_run}" == "true" ]]; then
            echo "Running in dry run mode."
          else
            echo "Running in normal mode."
          fi      
 
  copytocloudplatform:
    name: Copy to Cloud Platform
    needs: setup
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@0ad4b8fadaa221de15dcec353f45205ec38ea70b
        with:
          ref: ${{ github.ref }}

      - name: Get Account Id
        id: account_id
        run: |
          account_id="${{ fromJSON(secrets.MODERNISATION_PLATFORM_ENVIRONMENT_MANAGEMENT).account_ids.nomis-data-hub-production }}"
          echo "account_id=${account_id}"
          echo "account_id=${account_id}" >> $GITHUB_OUTPUT

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502
        with:
          role-to-assume: "arn:aws:iam::${{ steps.account_id.outputs.account_id }}:role/modernisation-platform-oidc-cicd"
          role-session-name: "github-${{ github.repository_id }}-${{ github.run_id }}"
          aws-region: eu-west-2

      - name: Test S3 Bucket Access
        env:
          SOURCE_BUCKET: ${{ secrets.OFFLOC_SOURCE_BUCKET }}
          DEST_BUCKET: ${{ secrets.OFFLOC_TRANSFER_S3_BUCKET_NAME_PREPROD }} # Preprod only at the moment
          DRY_RUN: ${{ needs.setup.outputs.dry_run }}
        run: |
          echo "Testing access to S3 buckets..."
          if [[ "${DRY_RUN}" == "true" ]]; then
            echo "[DRY RUN] Would test access to source bucket: ${SOURCE_BUCKET}"
            echo "[DRY RUN] Would test access to destination bucket: ${DEST_BUCKET}"
          else
            echo "Testing source bucket access..."
            aws s3 ls "s3://${SOURCE_BUCKET}/" --max-items 1
            echo "Testing destination bucket access..."
            aws s3 ls "s3://${DEST_BUCKET}/" --max-items 1
            echo "S3 bucket access confirmed"
          fi

      # - name: Check and Copy File
      #   env:
      #     SOURCE_BUCKET: ${{ secrets.OFFLOC_SOURCE_BUCKET }}
      #     DEST_BUCKET: ${{ secrets.secrets.OFFLOC_TRANSFER_S3_BUCKET_NAME_PREPROD }}
      #     DRY_RUN: ${{ needs.setup.outputs.dry_run }}
      #   run: |
      #     # Calculate cutoff time (23:00 local time of previous day)
      #     cutoff_time=$(TZ=Europe/London date -d "yesterday 23:00" -u +"%Y-%m-%dT%H:%M:%SZ")
      #     echo "Cutoff time (UTC): ${cutoff_time}"
      #     cutoff_epoch=$(date -d "${cutoff_time}" +%s)
          
      #     # Find the most recent file in source bucket
      #     echo "Searching for files in source bucket..."
      #     if [[ "${DRY_RUN}" == "true" ]]; then
      #       echo "[DRY RUN] Would search for files in s3://${SOURCE_BUCKET}/"
      #       echo "[DRY RUN] Would check if file timestamp is newer than ${cutoff_time}"
      #       echo "[DRY RUN] Would copy qualifying file to s3://${DEST_BUCKET}/"
      #       exit 0
      #     fi
          
      #     # Get list of files with timestamps
      #     files_info=$(aws s3 ls "s3://${SOURCE_BUCKET}/" --recursive | sort -k1,2)
          
      #     if [[ -z "${files_info}" ]]; then
      #       echo "No files found in source bucket"
      #       exit 1
      #     fi
          
      #     echo "Files found:"
      #     echo "${files_info}"
          
      #     # Find the most recent file that meets our criteria
      #     latest_file=""
      #     latest_timestamp=""
          
      #     while IFS= read -r line; do
      #       if [[ -n "${line}" ]]; then
      #         # Parse S3 ls output: date time size filename
      #         file_date=$(echo "${line}" | awk '{print $1}')
      #         file_time=$(echo "${line}" | awk '{print $2}')
      #         file_name=$(echo "${line}" | awk '{print $4}')
              
      #         # Convert to UTC timestamp for comparison
      #         file_datetime="${file_date}T${file_time}"
      #         file_epoch=$(date -d "${file_datetime}" +%s 2>/dev/null || echo "0")
              
      #         if [[ ${file_epoch} -gt ${cutoff_epoch} ]]; then
      #           if [[ -z "${latest_timestamp}" ]] || [[ ${file_epoch} -gt ${latest_timestamp} ]]; then
      #             latest_file="${file_name}"
      #             latest_timestamp=${file_epoch}
      #             echo "Found qualifying file: ${file_name} (${file_datetime})"
      #           fi
      #         fi
      #       fi
      #     done <<< "${files_info}"
          
      #     if [[ -z "${latest_file}" ]]; then
      #       echo "No files found newer than ${cutoff_time}"
      #       exit 0
      #     fi
          
      #     echo "Latest qualifying file: ${latest_file}"
      #     echo "Copying to destination bucket..."
          
      #     # Copy the file
      #     aws s3 cp "s3://${SOURCE_BUCKET}/${latest_file}" "s3://${DEST_BUCKET}/${latest_file}"
          
      #     if [[ $? -eq 0 ]]; then
      #       echo "Successfully copied ${latest_file} to destination bucket"
      #     else
      #       echo "Failed to copy file"
      #       exit 1
      #     fi
